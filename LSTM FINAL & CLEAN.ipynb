{"cells":[{"metadata":{},"cell_type":"markdown","source":"Credits to https://www.kaggle.com/bountyhunters/baseline-lstm-with-keras-0-7 for setup, general structure, and pieces of code"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# imports & guarantee reproducible values\nseed_value= 2\n\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\nimport random\nrandom.seed(seed_value)\n\nimport numpy as np\nnp.random.seed(seed_value)\n\nimport tensorflow as tf\ntf.random.set_seed(seed_value)\n\nfrom keras import backend as K\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)\n\nimport pandas as pd\nfrom scipy.sparse import csr_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataPath = \"/kaggle/input/m5-forecasting-accuracy/\"\n\n# Hyperparameters\ntimesteps = 15\nstartDay = 350\nRMSE = False\nEPOCH_NO = 32\n\n# Features\nDBE = True\nWDAY = True\nSTORE_AVG = False\nSTATE_AVG = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_eval = pd.read_csv(dataPath + \"/sales_train_evaluation.csv\")\ndt = dt_eval.iloc[:,0:1919]\ndt_eval = dt_eval.iloc[:,1919:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To reduce memory usage\ndef downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype('float32')\n    df[int_cols] = df[int_cols].astype('int16')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reduce memory usage\ndt = downcast_dtypes(dt)\ndt_eval = downcast_dtypes(dt_eval)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"### STORE AVERAGE ###\n# Calculate Store averages\nstore_avg = dt.groupby('store_id', as_index=False)[dt.columns[6:]].mean()\nstore_avg = store_avg.drop(store_avg.columns[0], axis=1)   \nstore_avg = store_avg.T\nstore_avg.columns = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\nstore_avg = store_avg[startDay:]\nstore_avg.index = dt.index[StartDay:1913]\n\n# Save store_id for prediction\nstore_id = pd.DataFrame(dt['store_id'])\nstore_id.columns = ['store_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### STATE AVERAGE ###\n# Calculate State averages\nstate_avg = dt.groupby('state_id', as_index=False)[dt.columns[6:]].mean()\nstate_avg = state_avg.drop(state_avg.columns[0], axis=1)   \nstate_avg = state_avg.T\nstate_avg.columns = ['CA', 'TX', 'WI']\nstate_avg = state_avg[startDay:]\nstate_avg.index = dt.index[StartDay:1913]\n\n# Save state_id column for prediction loop\nstate_id = pd.DataFrame(dt['state_id'])\nstate_id.columns = ['state_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Trasnpose to items as columns, Days as rows\ndt = dt.T    \ndt_eval = dt_eval.T\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove id, item_id, dept_id, cat_id, store_id, state_id columns\ndt = dt[6 + startDay:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv(dataPath + \"/calendar.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### DAY BEFORE EVENT ###\n\n#Create dataframe with zeros for 1969 days in the calendar\ndaysBeforeEvent = pd.DataFrame(np.zeros((1969,1)))\nfor x,y in calendar.iterrows():\n   if((pd.isnull(calendar[\"event_name_1\"][x])) == False):\n           daysBeforeEvent[0][x-1] = 1 \n\n#\"daysBeforeEventTest\" will be used as input for predicting (We will forecast the days 1913-1941)\ndaysBeforeEventTest = daysBeforeEvent[1913:1941]\n\n#\"daysBeforeEvent\" will be used for training as a feature.\ndaysBeforeEvent = daysBeforeEvent[startDay:1913]\n\n#Before concatanation with our main data \"dt\", indexes are made same and column name is changed to \"oneDayBeforeEvent\"\ndaysBeforeEvent.columns = [\"oneDayBeforeEvent\"]\ndaysBeforeEvent.index = dt.index[:1913]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### DAY OF THE WEEK ###\nweekday = pd.DataFrame(calendar[\"wday\"])\nweekdayTest = weekday[1913:1941]\nweekday = weekday[startDay:1913]\n\n# One Hot Encoding\nweekday = pd.get_dummies(weekday, columns=['wday'])\nweekdayTest = pd.get_dummies(weekdayTest, columns=['wday'])\n\nweekday.index = dt.index[:1913]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### ADD FEATURES ###\nif DBE:\n    dt = pd.concat([dt, daysBeforeEvent], axis = 1)\nif WDAY:\n    dt = pd.concat([dt, weekday], axis = 1)\nif STORE_AVG:\n    dt = pd.concat([dt, store_avg], axis = 1)\nif STATE_AVG:\n    dt = pd.concat([dt, state_avg], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\"calendar\" won't be used anymore. \ndel calendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Scaling\n#Scale the features using min-max scaler in range 0-1\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0, 1))\ndt_scaled = sc.fit_transform(dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = []\ny_train = []\nfor i in range(timesteps, 1913 - startDay):\n    X_train.append(dt_scaled[i-timesteps:i])\n    y_train.append(dt_scaled[i][0:30490]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del dt_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert to np array to be able to feed the LSTM model\nX_train = np.array(X_train)\ny_train = np.array(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\n\n# Initialising the RNN\nregressor = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\nlayer_1_units=40\nregressor.add(LSTM(units = layer_1_units, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\nregressor.add(Dropout(0.2))\n\n# Adding a second LSTM layer and some Dropout regularisation\nlayer_2_units=300\nregressor.add(LSTM(units = layer_2_units, return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a third LSTM layer and some Dropout regularisation\nlayer_3_units=300\nregressor.add(LSTM(units = layer_3_units))\nregressor.add(Dropout(0.2))\n\n# Adding the output layer\nregressor.add(Dense(units = 30490))\n\n# Compiling the RNN\nif RMSE:\n    regressor.compile(optimizer = 'adam', loss = root_mean_squared_error)\nelse:\n    regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nepoch_no=EPOCH_NO\nbatch_size_RNN=44\nregressor.fit(X_train, y_train, epochs = epoch_no, batch_size = batch_size_RNN, validation_split = 0.0)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs= dt[-timesteps:]\ninputs = sc.transform(inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = []\nX_test.append(inputs[0:timesteps])\nX_test = np.array(X_test)\npredictions = []\n\nfor j in range(timesteps,timesteps + 28):\n    predicted_stock_price = regressor.predict(X_test[0,j - timesteps:j].reshape(1, timesteps, inputs.shape[1]))\n    p_sales = np.array(predicted_stock_price).T\n    \n    # Calculate store average\n    store_avg = pd.DataFrame(np.zeros((30490,2)))\n    store_avg.columns = ['store_id','sales']\n    store_avg['store_id'] = store_id['store_id']\n    store_avg['sales'] = p_sales    \n    store_avg = store_avg.groupby('store_id', as_index=False)['sales'].mean()\n    store_avg = store_avg.drop('store_id', axis=1)  \n    store_avg = store_avg.T\n    \n    # Calculate state average\n    state_avg = pd.DataFrame(np.zeros((30490,2)))\n    state_avg.columns = ['state_id', 'sales']\n    state_avg['state_id'] = state_id\n    state_avg['sales'] = p_sales    \n    state_avg = state_avg.groupby('state_id', as_index=False)['sales'].mean()\n    state_avg = state_avg.drop('state_id', axis=1)   \n    state_avg = state_avg.T\n    \n    # Add sales\n    testInput = np.array(predicted_stock_price)\n    \n    # Add DayBeforeEvent\n    if DBE:\n        testInput = np.column_stack((testInput,daysBeforeEventTest[0][1913 + j - timesteps]))\n    \n    # Add weekday\n    if WDAY:\n        for i in range(1,8):\n            testInput = np.column_stack((testInput,weekdayTest['wday_'+str(i)][1913 + j - timesteps]))\n        \n    # Add store average\n    if STORE_AVG:\n        for i in range(0,10):\n            testInput = np.column_stack((testInput,store_avg[i]))\n        \n    # Add state average\n    if STATE_AVG:\n        for i in range(0,3):\n            testInput = np.column_stack((testInput,state_avg[i]))\n    \n    \n    X_test = np.append(X_test, testInput).reshape(1,j + 1,inputs.shape[1])\n    predicted_stock_price = sc.inverse_transform(testInput)[:,0:30490]\n    predictions.append(predicted_stock_price)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del regressor\ndel dt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WRMSSE\nCredits to https://www.kaggle.com/jeffzi/fast-clear-wrmsse-18ms for this part"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define fold pass here:\nfile_pass = '/kaggle/input/wrmsse/'# '/kaggle/input/fast-wrmsse-and-sw-frame/'\n\n# Load S and W weights for WRMSSE calcualtions:\nsw_df = pd.read_pickle(file_pass+'sw_df.pkl')\nS = sw_df.s.values\nW = sw_df.w.values\nSW = sw_df.sw.values\n\n# Load roll up matrix to calcualte aggreagates:\nroll_mat_df = pd.read_pickle(file_pass+'roll_mat_df.pkl')\nroll_index = roll_mat_df.index\nroll_mat_csr = csr_matrix(roll_mat_df.values)\n\ndel roll_mat_df\n\n# Function to do quick rollups:\ndef rollup(v):\n    '''\n    v - np.array of size (30490 rows, n day columns)\n    v_rolledup - array of size (n, 42840)\n    '''\n    return roll_mat_csr*v #(v.T*roll_mat_csr.T).T\n\n\n# Function to calculate WRMSSE:\ndef wrmsse(preds, y_true, score_only=True, s = S, w = W, sw=SW):\n    '''\n    preds - Predictions: pd.DataFrame of size (30490 rows, N day columns)\n    y_true - True values: pd.DataFrame of size (30490 rows, N day columns)\n    sequence_length - np.array of size (42840,)\n    sales_weight - sales weights based on last 28 days: np.array (42840,)\n    '''\n    \n    if score_only:\n        return np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(rollup(preds-y_true))\n                            ,axis=1)) * sw)/12 #<-used to be mistake here\n    else: \n        score_matrix = (np.square(rollup(preds-y_true)) * np.square(w)[:, None])/ s[:, None]\n        score = np.sum(np.sqrt(np.mean(score_matrix,axis=1)))/12 #<-used to be mistake here\n        return score, score_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = wrmsse(np.array(predictions).reshape((28,-1)).T, dt_eval.T)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nsubmission = pd.DataFrame(data=np.array(predictions).reshape(28,30490))\nsubmission = submission.T\nsubmission = pd.concat((submission, submission), ignore_index=True)\nsample_submission = pd.read_csv(dataPath + \"/sample_submission.csv\")\n    \nidColumn = sample_submission[[\"id\"]]\n    \nsubmission[[\"id\"]] = idColumn  \n\ncols = list(submission.columns)\ncols = cols[-1:] + cols[:-1]\nsubmission = submission[cols]\n\ncolsdeneme = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\n\nsubmission.columns = colsdeneme\n\ncurrentDateTime = time.strftime(\"%d%m%Y_%H%M%S\")\n\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}